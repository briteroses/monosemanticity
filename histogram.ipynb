{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    Iterable,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    Sequence,\n",
    "    Type,\n",
    "    Union,\n",
    "    List,\n",
    "    cast,\n",
    ")\n",
    "import interp.tools.optional as op\n",
    "import numpy as np\n",
    "import rust_circuit as rc\n",
    "import torch\n",
    "from interp.circuit.causal_scrubbing.experiment import (\n",
    "    Experiment,\n",
    "    ExperimentCheck,\n",
    "    ExperimentEvalSettings,\n",
    "    ScrubbedExperiment,\n",
    ")\n",
    "from interp.circuit.causal_scrubbing.hypothesis import (\n",
    "    Correspondence,\n",
    "    CondSampler,\n",
    "    ExactSampler,\n",
    "    FuncSampler,\n",
    "    InterpNode,\n",
    "    UncondSampler,\n",
    "    chain_excluding,\n",
    "    corr_root_matcher,\n",
    ")\n",
    "from interp.circuit.interop_rust.algebric_rewrite import (\n",
    "    residual_rewrite,\n",
    "    split_to_concat,\n",
    ")\n",
    "from interp.circuit.interop_rust.model_rewrites import To, configure_transformer\n",
    "from interp.circuit.interop_rust.module_library import load_model_id\n",
    "from interp.tools.indexer import TORCH_INDEXER as I\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "from interp.circuit.testing.notebook import NotebookInTesting\n",
    "from interp import cui\n",
    "from interp.ui.very_named_tensor import VeryNamedTensor\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "import random\n",
    "import re\n",
    "import functools\n",
    "\n",
    "import transformers\n",
    "from interp.tools.interpretability_tools import MODELS_DIR\n",
    "\n",
    "tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\n",
    "    f\"{MODELS_DIR}/gpt2/tokenizer\"\n",
    ")\n",
    "\n",
    "MODEL_ID = \"gelu_12_tied\"\n",
    "SEQ_LEN = 256\n",
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "PRINT_CIRCUITS = True\n",
    "ACTUALLY_RUN = True\n",
    "SLOW_EXPERIMENTS = True\n",
    "DEFAULT_CHECKS: ExperimentCheck = True\n",
    "EVAL_DEVICE = \"cuda:0\"\n",
    "MAX_MEMORY = 80000000000\n",
    "BATCH_SIZE = 32\n",
    "MLP_WIDTH = 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"gelu_12_tied\"  # aka gpt2 small\n",
    "circ_dict, _, model_info = load_model_id(MODEL_ID)\n",
    "unbound_circuit = circ_dict[\"t.bind_w\"]\n",
    "\n",
    "tokens_arr = rc.Symbol.new_with_random_uuid((SEQ_LEN,), name=\"tokens\")\n",
    "# We use this to index into the tok_embeds to get the proper embeddings\n",
    "token_embeds = rc.GeneralFunction.gen_index(\n",
    "    circ_dict[\"t.w.tok_embeds\"], tokens_arr, 0, name=\"tok_embeds\"\n",
    ")\n",
    "bound_circuit = model_info.bind_to_input(\n",
    "    unbound_circuit, token_embeds, circ_dict[\"t.w.pos_embeds\"]\n",
    ")\n",
    "\n",
    "transformed_circuit = rc.conform_all_modules(bound_circuit)\n",
    "subbed_circuit = transformed_circuit\n",
    "\n",
    "\n",
    "def module_but_norm(circuit: rc.Circuit):\n",
    "    if isinstance(circuit, rc.Module):\n",
    "        if \"norm\" in circuit.name or \"ln\" in circuit.name or \"final\" in circuit.name:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    subbed_circuit = subbed_circuit.update(\n",
    "        module_but_norm, lambda c: c.cast_module().substitute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the neuron activations and create evaluation schedule\n",
    "\n",
    "dd = rc.TorchDeviceDtype(EVAL_DEVICE, torch.float32)\n",
    "batched_circuit = rc.cast_circuit(\n",
    "    rc.Expander(\n",
    "        (\n",
    "            \"tokens\",\n",
    "            lambda _: rc.Symbol.new_with_random_uuid(\n",
    "                (BATCH_SIZE, SEQ_LEN),\n",
    "                name=\"tokens\",\n",
    "            ),\n",
    "        )\n",
    "    )(subbed_circuit),\n",
    "    dd.op(),\n",
    ")\n",
    "\n",
    "layer_activations = [\n",
    "    batched_circuit.get_unique(\n",
    "        rc.IterativeMatcher(f\"b{i}.call\").chain(\n",
    "            rc.restrict(rc.IterativeMatcher(\"m.act\"), end_depth=5)\n",
    "        )\n",
    "    )\n",
    "    for i in range(12)\n",
    "]\n",
    "\n",
    "schedule = rc.optimize_to_schedule(\n",
    "    rc.Concat.stack(*layer_activations, axis=1),\n",
    "    rc.OptimizationSettings(\n",
    "        max_memory=MAX_MEMORY,\n",
    "        max_single_tensor_memory=MAX_MEMORY,\n",
    "        device_dtype=dd,\n",
    "    ),\n",
    ")\n",
    "token_hash = batched_circuit.get_unique(\"tokens\").cast_symbol().hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset openwebtext (/home/ubuntu/.cache/huggingface/datasets/openwebtext/plain_text/1.0.0/5c636399c7155da97c982d0d70ecdce30fbca66a4eb4fc768ad91f8331edac02)\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/openwebtext/plain_text/1.0.0/5c636399c7155da97c982d0d70ecdce30fbca66a4eb4fc768ad91f8331edac02/cache-6fdd753d61517566.arrow\n"
     ]
    }
   ],
   "source": [
    "# Load dataset tokens\n",
    "\n",
    "dataset = load_dataset(\"openwebtext\", split=\"train\")\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|endoftext|>\"})\n",
    "\n",
    "batch_text = dataset[\"text\"][0 : 0 + BATCH_SIZE]\n",
    "batch_tokens = tokenizer(\n",
    "    batch_text, return_tensors=\"pt\", padding=True, truncation=True\n",
    ")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tensor shape doesn't match entry shape!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Replace input with tokens and run\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     neurons \u001b[39m=\u001b[39m (\n\u001b[0;32m----> 5\u001b[0m         schedule\u001b[39m.\u001b[39;49mreplace_tensors({token_hash: batch_tokens\u001b[39m.\u001b[39;49mcuda()})\u001b[39m.\u001b[39mevaluate()\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m      6\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tensor shape doesn't match entry shape!"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Replace input with tokens and run\n",
    "\n",
    "with torch.no_grad():\n",
    "    neurons = (\n",
    "        schedule.replace_tensors({token_hash: batch_tokens.cuda()}).evaluate().cpu()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot them in the CUI\n",
    "\n",
    "if not cui.is_port_in_use(6789):\n",
    "    await cui.init(port=6789)\n",
    "\n",
    "N_TEXT_TOKENS = 10000\n",
    "\n",
    "mask = batch_tokens != tokenizer.pad_token_id\n",
    "tokens_concat = batch_tokens[mask]\n",
    "text_concat = tokenizer.batch_decode(tokens_concat[:N_TEXT_TOKENS])\n",
    "\n",
    "neuron_idxs = slice(100)\n",
    "firings = neurons.permute((1, 3, 0, 2))[:, neuron_idxs, :, :][:, :, mask]\n",
    "\n",
    "vnt = VeryNamedTensor(\n",
    "    firings[:, :, :N_TEXT_TOKENS],\n",
    "    dim_names=[\"layer\", \"neuron\", \"text\"],\n",
    "    dim_types=[\"head\", \"head\", \"seq\"],\n",
    "    dim_idx_names=[\n",
    "        tuple(map(str, range(12))),\n",
    "        tuple(map(lambda x: str(x.item()), torch.arange(3072)[neuron_idxs])),\n",
    "        text_concat,\n",
    "    ],\n",
    "    title=\"firings\",\n",
    ")\n",
    "\n",
    "await cui.show_tensors(vnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try to find some neurons that e.g. encode newline\n",
    "\n",
    "all_tokens = list(enumerate(tokenizer.batch_decode(range(tokenizer.pad_token_id))))\n",
    "all_tokens.sort(key=lambda x: x[1])\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between token==newline and various neurons\n",
    "\n",
    "def try_concept(fn):\n",
    "    is_newline = fn(tokens_concat)\n",
    "\n",
    "    all_vars = torch.stack(\n",
    "        [\n",
    "            torch.corrcoef(\n",
    "                torch.cat([is_newline.to(torch.float32)[None, :], firings[i]], dim=0)\n",
    "            )\n",
    "            for i in range(firings.shape[0])\n",
    "        ],\n",
    "        dim=0,\n",
    "    )\n",
    "\n",
    "    vs = all_vars[:, 1:, 0].reshape(-1)\n",
    "    topk = torch.topk(vs.abs(), 10)\n",
    "    return list(\n",
    "        (a.item(), b.item())\n",
    "        for (a, b) in zip(topk.indices % 3072, topk.indices // 3072)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newlines concept\n",
    "\n",
    "# The three newline tokens are\n",
    "# (198, '\\n'),\n",
    "# (628, '\\n\\n'),\n",
    "# (44320, '\\n\\xa0'),\n",
    "\n",
    "# try_concept(\n",
    "#     lambda tokens_concat: (tokens_concat == 198)\n",
    "#     | (tokens_concat == 628)\n",
    "#     | (tokens_concat == 44320)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preposition_list = \"\"\"\n",
    "about\n",
    "above\n",
    "across\n",
    "after\n",
    "against\n",
    "among\n",
    "around\n",
    "at\n",
    "before\n",
    "behind\n",
    "below\n",
    "beside\n",
    "between\n",
    "by\n",
    "down\n",
    "during\n",
    "for\n",
    "from\n",
    "in\n",
    "inside\n",
    "into\n",
    "near\n",
    "of\n",
    "off\n",
    "on\n",
    "out\n",
    "over\n",
    "through\n",
    "to\n",
    "toward\n",
    "under\n",
    "up\n",
    "with\n",
    "\"\"\".strip().split()\n",
    "\n",
    "\n",
    "preposition_tokens = [\n",
    "    (i, t)\n",
    "    for (i, t) in all_tokens\n",
    "    if any(re.match(f\"^ *{p} *$\", t.lower()) for p in preposition_list)\n",
    "]\n",
    "\n",
    "\n",
    "def is_preposition(array: torch.Tensor) -> torch.Tensor:\n",
    "    m = array == preposition_tokens[0][0]\n",
    "    for i, _ in preposition_tokens[1:]:\n",
    "        m |= array == i\n",
    "    return m\n",
    "\n",
    "\n",
    "try_concept(is_preposition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ad467ea6ce724cf68e3e9460a7b5d30a577b9574f7d768503ed98371d079a52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
